{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "from utils_project import generate_csv,create_dataframe_from_xyz_files,create_X_y_from_dataframe\n",
    "\n",
    "\n",
    "csv_path = \"../../data/energies/train.csv\"\n",
    "path_data = \"../../data/atoms/train\"\n",
    "df_train=create_dataframe_from_xyz_files(path_data,csv_path)\n",
    "X=df_train[['positions', 'energy', 'charges']]\n",
    "\n",
    "qm7 = X.to_dict(\"list\")\n",
    "\n",
    "#qm7 = fetch_qm7(align=True)\n",
    "pos = np.array(qm7['positions'])\n",
    "full_charges = np.array(qm7['charges'])\n",
    "\n",
    "n_molecules = pos.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ElementwiseProd(nn.Module):\n",
    "    def __init__(self, input_dim, q, k, act='sigmoid'):\n",
    "        super().__init__()\n",
    "        self.q = q\n",
    "        self.k = k\n",
    "        \n",
    "        # Sélection de la fonction d'activation\n",
    "        if act == 'sigmoid':\n",
    "            self.activation = torch.sigmoid\n",
    "        elif act == 'tanh':\n",
    "            self.activation = torch.tanh\n",
    "        elif act == 'relu':\n",
    "            self.activation = F.relu\n",
    "        else:\n",
    "            raise ValueError(f\"Activation '{act}' non supportée.\")\n",
    "        \n",
    "        # Création des k couches linéaires\n",
    "        self.hidden_layers = nn.ModuleList([\n",
    "            nn.Linear(input_dim, q) for _ in range(k)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = torch.ones(x.size(0), self.q, device=x.device)\n",
    "        for layer in self.hidden_layers:\n",
    "            out = self.activation(layer(x))\n",
    "            output *= out  # Produit élément par élément\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class ElementwiseProdRegressor(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, input_dim=1, q=10, k=3, act='sigmoid', epochs=100, lr=1e-3, verbose=False):\n",
    "        self.input_dim = input_dim\n",
    "        self.q = q\n",
    "        self.k = k\n",
    "        self.act = act\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "        self.verbose = verbose\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        class FullModel(nn.Module):\n",
    "            def __init__(self, input_dim, q, k, act):\n",
    "                super().__init__()\n",
    "                self.core = ElementwiseProd(input_dim, q, k, act)\n",
    "                self.output = nn.Linear(q, 1)\n",
    "            \n",
    "            def forward(self, x):\n",
    "                x = self.core(x)\n",
    "                x = self.output(x)\n",
    "                return x\n",
    "        \n",
    "        self.model = FullModel(self.input_dim, self.q, self.k, self.act).to(self.device)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = torch.tensor(X, dtype=torch.float32).to(self.device)\n",
    "        y = torch.tensor(y, dtype=torch.float32).view(-1, 1).to(self.device)\n",
    "\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "\n",
    "        self.model.train()\n",
    "        for epoch in range(self.epochs):\n",
    "            optimizer.zero_grad()\n",
    "            output = self.model(X)\n",
    "            loss = criterion(output, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if self.verbose and epoch % 10 == 0:\n",
    "                print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        self.model.eval()\n",
    "        X = torch.tensor(X, dtype=torch.float32).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            output = self.model(X)\n",
    "        return output.cpu().numpy().flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "M, N, O = 64, 64, 64 #192, 128, 96\n",
    "grille = \"64-64-64\"\n",
    "grid = np.mgrid[-M//2:-M//2+M, -N//2:-N//2+N, -O//2:-O//2+O]\n",
    "grid = np.fft.ifftshift(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_data = torch.load( f'../models_scattering/scattering_outputs_{grille}.pt', map_location=device)\n",
    "order_0 = saved_data['order_0']\n",
    "orders_1_and_2 = saved_data['orders_1_and_2']\n",
    "order_0 = order_0.cpu().numpy()\n",
    "orders_1_and_2 = orders_1_and_2.cpu().numpy()\n",
    "\n",
    "order_0 = order_0.reshape((n_molecules, -1))\n",
    "orders_1_and_2 = orders_1_and_2.reshape((n_molecules, -1))\n",
    "scattering_coef = np.concatenate([order_0, orders_1_and_2], axis=1)\n",
    "target = qm7['energy']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_folds = 3\n",
    "\n",
    "P = np.random.permutation(n_molecules).reshape((n_folds, -1))\n",
    "\n",
    "cross_val_folds = []\n",
    "\n",
    "for i_fold in range(n_folds):\n",
    "    fold = (np.concatenate(P[np.arange(n_folds) != i_fold], axis=0),\n",
    "            P[i_fold])\n",
    "    cross_val_folds.append(fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tristan/.local/lib/python3.10/site-packages/sklearn/linear_model/_ridge.py:204: LinAlgWarning: Ill-conditioned matrix (rcond=2.69884e-08): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, assume_a=\"pos\", overwrite_a=True).T\n",
      "/home/tristan/.local/lib/python3.10/site-packages/sklearn/linear_model/_ridge.py:204: LinAlgWarning: Ill-conditioned matrix (rcond=2.36013e-08): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, assume_a=\"pos\", overwrite_a=True).T\n",
      "/home/tristan/.local/lib/python3.10/site-packages/sklearn/linear_model/_ridge.py:204: LinAlgWarning: Ill-conditioned matrix (rcond=2.62322e-08): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, assume_a=\"pos\", overwrite_a=True).T\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge Regression with alpha=0.1: MAE: 0.27973338621637334, RMSE: 0.5729401825905865\n",
      "PyTorch ElementwiseProd: MAE: 3.915330205589707, RMSE: 5.456448035003863\n",
      "Le meilleur modèle est Ridge Regression with alpha=0.1 avec un RMSE de 0.5729401825905865.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import linear_model, preprocessing, pipeline, model_selection\n",
    "import joblib\n",
    "\n",
    "\n",
    "models = [\n",
    "    (\"Ridge Regression with alpha=0.1\", linear_model.Ridge(alpha=0.1)),\n",
    "    (\"PyTorch ElementwiseProd\", ElementwiseProdRegressor(input_dim=scattering_coef.shape[1], q=1000, k=3, epochs=50, lr=1e-2))\n",
    "\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "for name, model in models:\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "    regressor = pipeline.make_pipeline(scaler, model)\n",
    "\n",
    "    target_prediction = model_selection.cross_val_predict(regressor, X=scattering_coef, y=target, cv=cross_val_folds)\n",
    "\n",
    "    MAE = np.mean(np.abs(target_prediction - target))\n",
    "    RMSE = np.sqrt(np.mean((target_prediction - target) ** 2))\n",
    "\n",
    "    results.append((name, model, MAE, RMSE))\n",
    "\n",
    "    print('{}: MAE: {}, RMSE: {}'.format(name, MAE, RMSE))\n",
    "\n",
    "# Trouver le modèle avec le RMSE le plus bas\n",
    "best_result = min(results, key=lambda x: x[3])\n",
    "best_model_name, best_model, best_mae, best_rmse = best_result\n",
    "\n",
    "print(f\"Le meilleur modèle est {best_model_name} avec un RMSE de {best_rmse}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 6215.8677\n",
      "Epoch 10, Loss: 726.8629\n",
      "Epoch 20, Loss: 152.6881\n",
      "Epoch 30, Loss: 50.8605\n",
      "Epoch 40, Loss: 28.3487\n",
      "Epoch 50, Loss: 20.1895\n",
      "Epoch 60, Loss: 16.9228\n",
      "Epoch 70, Loss: 14.2303\n",
      "Epoch 80, Loss: 12.2828\n",
      "Epoch 90, Loss: 10.7554\n",
      "Epoch 100, Loss: 9.5177\n",
      "Epoch 110, Loss: 8.5330\n",
      "Epoch 120, Loss: 7.7063\n",
      "Epoch 130, Loss: 7.0017\n",
      "Epoch 140, Loss: 6.3881\n",
      "Epoch 150, Loss: 5.8741\n",
      "Epoch 160, Loss: 5.4344\n",
      "Epoch 170, Loss: 5.0500\n",
      "Epoch 180, Loss: 4.7112\n",
      "Epoch 190, Loss: 4.4038\n",
      "Epoch 200, Loss: 4.1269\n",
      "Epoch 210, Loss: 3.8759\n",
      "Epoch 220, Loss: 3.6473\n",
      "Epoch 230, Loss: 3.4327\n",
      "Epoch 240, Loss: 3.2311\n",
      "Epoch 250, Loss: 3.0462\n",
      "Epoch 260, Loss: 2.8715\n",
      "Epoch 270, Loss: 2.7102\n",
      "Epoch 280, Loss: 2.5597\n",
      "Epoch 290, Loss: 2.4116\n",
      "Epoch 300, Loss: 2.2982\n",
      "Epoch 310, Loss: 2.1606\n",
      "Epoch 320, Loss: 2.0495\n",
      "Epoch 330, Loss: 1.9431\n",
      "Epoch 340, Loss: 1.8585\n",
      "Epoch 350, Loss: 1.7932\n",
      "Epoch 360, Loss: 1.6675\n",
      "Epoch 370, Loss: 1.5983\n",
      "Epoch 380, Loss: 1.5210\n",
      "Epoch 390, Loss: 1.4589\n",
      "Epoch 400, Loss: 1.4589\n",
      "Epoch 410, Loss: 1.3863\n",
      "Epoch 420, Loss: 1.2742\n",
      "Epoch 430, Loss: 1.2165\n",
      "Epoch 440, Loss: 1.1680\n",
      "Epoch 450, Loss: 1.1104\n",
      "Epoch 460, Loss: 1.2690\n",
      "Epoch 470, Loss: 1.0963\n",
      "Epoch 480, Loss: 1.0021\n",
      "Epoch 490, Loss: 0.9643\n",
      "Epoch 500, Loss: 0.9208\n",
      "Epoch 510, Loss: 0.8887\n",
      "Epoch 520, Loss: 0.8764\n",
      "Epoch 530, Loss: 1.0583\n",
      "Epoch 540, Loss: 0.9007\n",
      "Epoch 550, Loss: 0.8082\n",
      "Epoch 560, Loss: 0.7528\n",
      "Epoch 570, Loss: 0.7250\n",
      "Epoch 580, Loss: 0.7004\n",
      "Epoch 590, Loss: 0.7036\n",
      "Epoch 600, Loss: 0.7498\n",
      "Epoch 610, Loss: 0.8384\n",
      "Epoch 620, Loss: 0.6496\n",
      "Epoch 630, Loss: 0.5851\n",
      "Epoch 640, Loss: 0.5913\n",
      "Epoch 650, Loss: 0.5729\n",
      "Epoch 660, Loss: 0.5358\n",
      "Epoch 670, Loss: 0.5643\n",
      "Epoch 680, Loss: 0.6237\n",
      "Epoch 690, Loss: 0.5251\n",
      "Epoch 700, Loss: 0.4883\n",
      "Epoch 710, Loss: 0.4727\n",
      "Epoch 720, Loss: 0.4588\n",
      "Epoch 730, Loss: 0.4324\n",
      "Epoch 740, Loss: 0.4611\n",
      "Epoch 750, Loss: 0.4017\n",
      "Epoch 760, Loss: 0.5036\n",
      "Epoch 770, Loss: 0.4888\n",
      "Epoch 780, Loss: 0.3698\n",
      "Epoch 790, Loss: 0.3934\n",
      "Epoch 800, Loss: 0.3590\n",
      "Epoch 810, Loss: 0.3564\n",
      "Epoch 820, Loss: 0.3518\n",
      "Epoch 830, Loss: 0.3997\n",
      "Epoch 840, Loss: 0.3423\n",
      "Epoch 850, Loss: 0.4183\n",
      "Epoch 860, Loss: 0.3617\n",
      "Epoch 870, Loss: 0.2922\n",
      "Epoch 880, Loss: 0.2850\n",
      "Epoch 890, Loss: 0.2939\n",
      "Epoch 900, Loss: 0.2722\n",
      "Epoch 910, Loss: 0.5198\n",
      "Epoch 920, Loss: 0.2750\n",
      "Epoch 930, Loss: 0.3001\n",
      "Epoch 940, Loss: 0.2548\n",
      "Epoch 950, Loss: 0.2585\n",
      "Epoch 960, Loss: 0.2605\n",
      "Epoch 970, Loss: 0.2313\n",
      "Epoch 980, Loss: 0.2623\n",
      "Epoch 990, Loss: 0.2382\n",
      "Epoch 0, Loss: 6186.9458\n",
      "Epoch 10, Loss: 736.4509\n",
      "Epoch 20, Loss: 166.3850\n",
      "Epoch 30, Loss: 58.5330\n",
      "Epoch 40, Loss: 30.4875\n",
      "Epoch 50, Loss: 21.4231\n",
      "Epoch 60, Loss: 18.0741\n",
      "Epoch 70, Loss: 15.0871\n",
      "Epoch 80, Loss: 12.9703\n",
      "Epoch 90, Loss: 11.3290\n",
      "Epoch 100, Loss: 10.0632\n",
      "Epoch 110, Loss: 9.0273\n",
      "Epoch 120, Loss: 8.1747\n",
      "Epoch 130, Loss: 7.4692\n",
      "Epoch 140, Loss: 6.8765\n",
      "Epoch 150, Loss: 6.3632\n",
      "Epoch 160, Loss: 5.9087\n",
      "Epoch 170, Loss: 5.5068\n",
      "Epoch 180, Loss: 5.1396\n",
      "Epoch 190, Loss: 4.8093\n",
      "Epoch 200, Loss: 4.5018\n",
      "Epoch 210, Loss: 4.2269\n",
      "Epoch 220, Loss: 3.9800\n",
      "Epoch 230, Loss: 3.7477\n",
      "Epoch 240, Loss: 3.5267\n",
      "Epoch 250, Loss: 3.3615\n",
      "Epoch 260, Loss: 3.2021\n",
      "Epoch 270, Loss: 3.0153\n",
      "Epoch 280, Loss: 2.8362\n",
      "Epoch 290, Loss: 2.6797\n",
      "Epoch 300, Loss: 2.5347\n",
      "Epoch 310, Loss: 2.4344\n",
      "Epoch 320, Loss: 2.2928\n",
      "Epoch 330, Loss: 2.1852\n",
      "Epoch 340, Loss: 2.1769\n",
      "Epoch 350, Loss: 1.9948\n",
      "Epoch 360, Loss: 1.9442\n",
      "Epoch 370, Loss: 1.8271\n",
      "Epoch 380, Loss: 1.7847\n",
      "Epoch 390, Loss: 1.6707\n",
      "Epoch 400, Loss: 1.6239\n",
      "Epoch 410, Loss: 1.5903\n",
      "Epoch 420, Loss: 1.5321\n",
      "Epoch 430, Loss: 1.4137\n",
      "Epoch 440, Loss: 1.4181\n",
      "Epoch 450, Loss: 1.3386\n",
      "Epoch 460, Loss: 1.2916\n",
      "Epoch 470, Loss: 1.3239\n",
      "Epoch 480, Loss: 1.3908\n",
      "Epoch 490, Loss: 1.1490\n",
      "Epoch 500, Loss: 1.1131\n",
      "Epoch 510, Loss: 1.0571\n",
      "Epoch 520, Loss: 1.0171\n",
      "Epoch 530, Loss: 1.1384\n",
      "Epoch 540, Loss: 1.1170\n",
      "Epoch 550, Loss: 0.9680\n",
      "Epoch 560, Loss: 0.9366\n",
      "Epoch 570, Loss: 0.8943\n",
      "Epoch 580, Loss: 0.9263\n",
      "Epoch 590, Loss: 0.8808\n",
      "Epoch 600, Loss: 0.9710\n",
      "Epoch 610, Loss: 0.8078\n",
      "Epoch 620, Loss: 0.7617\n",
      "Epoch 630, Loss: 0.7552\n",
      "Epoch 640, Loss: 0.8202\n",
      "Epoch 650, Loss: 0.8907\n",
      "Epoch 660, Loss: 0.7074\n",
      "Epoch 670, Loss: 0.6701\n",
      "Epoch 680, Loss: 0.6749\n",
      "Epoch 690, Loss: 0.7281\n",
      "Epoch 700, Loss: 0.8011\n",
      "Epoch 710, Loss: 0.6107\n",
      "Epoch 720, Loss: 0.5943\n",
      "Epoch 730, Loss: 0.6083\n",
      "Epoch 740, Loss: 0.7084\n",
      "Epoch 750, Loss: 0.5573\n",
      "Epoch 760, Loss: 0.5545\n",
      "Epoch 770, Loss: 0.5286\n",
      "Epoch 780, Loss: 0.5901\n",
      "Epoch 790, Loss: 0.5596\n",
      "Epoch 800, Loss: 0.5615\n",
      "Epoch 810, Loss: 0.5427\n",
      "Epoch 820, Loss: 0.5033\n",
      "Epoch 830, Loss: 0.4609\n",
      "Epoch 840, Loss: 0.5889\n",
      "Epoch 850, Loss: 0.5661\n",
      "Epoch 860, Loss: 0.5279\n",
      "Epoch 870, Loss: 0.4805\n",
      "Epoch 880, Loss: 0.4329\n",
      "Epoch 890, Loss: 0.4317\n",
      "Epoch 900, Loss: 0.5348\n",
      "Epoch 910, Loss: 0.4927\n",
      "Epoch 920, Loss: 0.4485\n",
      "Epoch 930, Loss: 0.4007\n",
      "Epoch 940, Loss: 0.3702\n",
      "Epoch 950, Loss: 0.4609\n",
      "Epoch 960, Loss: 0.4598\n",
      "Epoch 970, Loss: 0.4147\n",
      "Epoch 980, Loss: 0.3653\n",
      "Epoch 990, Loss: 0.3347\n",
      "Epoch 0, Loss: 6196.0762\n",
      "Epoch 10, Loss: 725.2909\n",
      "Epoch 20, Loss: 155.0097\n",
      "Epoch 30, Loss: 53.3365\n",
      "Epoch 40, Loss: 29.2972\n",
      "Epoch 50, Loss: 20.9948\n",
      "Epoch 60, Loss: 17.4622\n",
      "Epoch 70, Loss: 14.2841\n",
      "Epoch 80, Loss: 12.1282\n",
      "Epoch 90, Loss: 10.4153\n",
      "Epoch 100, Loss: 9.1159\n",
      "Epoch 110, Loss: 8.1170\n",
      "Epoch 120, Loss: 7.3124\n",
      "Epoch 130, Loss: 6.6664\n",
      "Epoch 140, Loss: 6.1366\n",
      "Epoch 150, Loss: 5.6890\n",
      "Epoch 160, Loss: 5.3116\n",
      "Epoch 170, Loss: 4.9778\n",
      "Epoch 180, Loss: 4.6760\n",
      "Epoch 190, Loss: 4.4011\n",
      "Epoch 200, Loss: 4.1486\n",
      "Epoch 210, Loss: 3.9170\n",
      "Epoch 220, Loss: 3.7003\n",
      "Epoch 230, Loss: 3.5009\n",
      "Epoch 240, Loss: 3.3158\n",
      "Epoch 250, Loss: 3.1405\n",
      "Epoch 260, Loss: 2.9751\n",
      "Epoch 270, Loss: 2.8188\n",
      "Epoch 280, Loss: 2.6941\n",
      "Epoch 290, Loss: 2.5613\n",
      "Epoch 300, Loss: 2.4370\n",
      "Epoch 310, Loss: 2.3003\n",
      "Epoch 320, Loss: 2.2112\n",
      "Epoch 330, Loss: 2.1614\n",
      "Epoch 340, Loss: 2.0722\n",
      "Epoch 350, Loss: 1.9306\n",
      "Epoch 360, Loss: 1.8161\n",
      "Epoch 370, Loss: 1.7367\n",
      "Epoch 380, Loss: 1.6773\n",
      "Epoch 390, Loss: 1.6401\n",
      "Epoch 400, Loss: 1.6118\n",
      "Epoch 410, Loss: 1.5359\n",
      "Epoch 420, Loss: 1.3942\n",
      "Epoch 430, Loss: 1.3462\n",
      "Epoch 440, Loss: 1.3540\n",
      "Epoch 450, Loss: 1.3563\n",
      "Epoch 460, Loss: 1.2303\n",
      "Epoch 470, Loss: 1.1384\n",
      "Epoch 480, Loss: 1.1472\n",
      "Epoch 490, Loss: 1.0989\n",
      "Epoch 500, Loss: 1.1808\n",
      "Epoch 510, Loss: 0.9762\n",
      "Epoch 520, Loss: 0.9776\n",
      "Epoch 530, Loss: 0.9421\n",
      "Epoch 540, Loss: 0.9586\n",
      "Epoch 550, Loss: 0.9075\n",
      "Epoch 560, Loss: 0.8667\n",
      "Epoch 570, Loss: 0.8387\n",
      "Epoch 580, Loss: 0.8203\n",
      "Epoch 590, Loss: 0.9401\n",
      "Epoch 600, Loss: 0.7212\n",
      "Epoch 610, Loss: 0.7075\n",
      "Epoch 620, Loss: 0.6683\n",
      "Epoch 630, Loss: 0.7328\n",
      "Epoch 640, Loss: 0.6835\n",
      "Epoch 650, Loss: 0.6755\n",
      "Epoch 660, Loss: 0.6756\n",
      "Epoch 670, Loss: 0.6952\n",
      "Epoch 680, Loss: 0.7119\n",
      "Epoch 690, Loss: 0.5580\n",
      "Epoch 700, Loss: 0.5315\n",
      "Epoch 710, Loss: 0.6167\n",
      "Epoch 720, Loss: 0.7226\n",
      "Epoch 730, Loss: 0.5185\n",
      "Epoch 740, Loss: 0.5164\n",
      "Epoch 750, Loss: 0.5086\n",
      "Epoch 760, Loss: 0.4673\n",
      "Epoch 770, Loss: 0.4483\n",
      "Epoch 780, Loss: 0.4466\n",
      "Epoch 790, Loss: 0.4417\n",
      "Epoch 800, Loss: 0.4127\n",
      "Epoch 810, Loss: 0.4560\n",
      "Epoch 820, Loss: 0.4017\n",
      "Epoch 830, Loss: 0.4194\n",
      "Epoch 840, Loss: 0.4728\n",
      "Epoch 850, Loss: 0.5027\n",
      "Epoch 860, Loss: 0.4616\n",
      "Epoch 870, Loss: 0.4439\n",
      "Epoch 880, Loss: 0.4158\n",
      "Epoch 890, Loss: 0.3826\n",
      "Epoch 900, Loss: 0.3567\n",
      "Epoch 910, Loss: 0.3275\n",
      "Epoch 920, Loss: 0.3272\n",
      "Epoch 930, Loss: 0.3485\n",
      "Epoch 940, Loss: 0.4058\n",
      "Epoch 950, Loss: 0.3891\n",
      "Epoch 960, Loss: 0.3907\n",
      "Epoch 970, Loss: 0.3654\n",
      "Epoch 980, Loss: 0.3245\n",
      "Epoch 990, Loss: 0.2916\n",
      "PyTorch ElementwiseProd: MAE: 0.4824701126275599, RMSE: 1.0871054336355932\n"
     ]
    }
   ],
   "source": [
    "model = ElementwiseProdRegressor(input_dim=scattering_coef.shape[1], q=3000, k=3, epochs=1000, lr=1e-2, verbose=True)\n",
    "\n",
    "scaler = preprocessing.StandardScaler()\n",
    "regressor = pipeline.make_pipeline(scaler, model)\n",
    "target_prediction = model_selection.cross_val_predict(regressor, X=scattering_coef, y=target, cv=cross_val_folds)\n",
    "\n",
    "MAE = np.mean(np.abs(target_prediction - target))\n",
    "RMSE = np.sqrt(np.mean((target_prediction - target) ** 2))\n",
    "\n",
    "print('{}: MAE: {}, RMSE: {}'.format(name, MAE, RMSE))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
